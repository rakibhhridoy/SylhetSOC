


import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from mpl_toolkits.mplot3d import Axes3D
from scipy.interpolate import griddata
warnings.filterwarnings("ignore", category=UserWarning)


pt = pd.read_csv("data/PresentTopSoil.csv")
ps = pd.read_csv("data/PresentSubSoil.csv")


pvt = pd.read_csv("data/PreviousTopSoil.csv")
pvs = pd.read_csv("data/PreviousSubSoil.csv")


pt["Year"] = 2025
pvt["Year"] = 1985


merged = pd.concat([pt, pvt], axis= 0)
merged


merged.to_csv("data/TopSoil.csv", index=False)


merged.columns


pt


!tree gis


pt.columns


import ee
import geopandas as gpd
from datetime import datetime, timedelta
import os

# AuthenticateGEE (only needs to be done once)
ee.Authenticate()


!earthengine authenticate --force


ee.Initialize(project="ee-arsenicbd")


import ee
import geopandas as gpd
from datetime import datetime, timedelta

shapefile_path = "gis/StudyArea.shp"
gdf = gpd.read_file(shapefile_path)

# Get the geometry of the shapefile and convert to geoJSON format
geometry = gdf.geometry.union_all()  # Using union_all() instead of unary_union
geometry_geojson = geometry.__geo_interface__


# Convert GeoJSON geometry into Earth Engine Geometry (ensure proper structure)
study_area = ee.Geometry.Polygon(geometry_geojson['coordinates'][0])

# Define the time period (last 5 years)
end_date = datetime.today()
start_date = end_date - timedelta(days=5*365)

# Convert start and end dates to strings in the required format
start_date_str = start_date.strftime('%Y-%m-%d')
end_date_str = end_date.strftime('%Y-%m-%d')

# Define the Sentinel-2 ImageCollection
sentinel2 = ee.ImageCollection("COPERNICUS/S2") \
    .filterBounds(study_area) \
    .filterDate(ee.Date(start_date_str), ee.Date(end_date_str)) \
    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 30))  # Filter to less than 30% cloud cover

# Select relevant bands (e.g., B4, B3, B2, B8 for Red, Green, Blue, NIR)
sentinel2 = sentinel2.select(['B4', 'B3', 'B2', 'B8'])

# Function to apply a cloud mask using NDVI-based approach or SCL
def mask_clouds(image):
    # Use NDVI or SCL-based cloud mask (SCL is available in Level 2A, or use a threshold for NDVI)
    # First calculate the NDVI for cloud masking
    ndvi = image.normalizedDifference(['B8', 'B4']).rename('NDVI')
    
    # Create a cloud mask: NDVI less than a certain threshold will likely be cloudy
    cloud_mask = ndvi.gt(0.2)  # Use NDVI > 0.2 for non-cloudy pixels (this is a threshold you can adjust)
    
    # Mask out clouds
    return image.updateMask(cloud_mask)

# Apply the cloud mask to all images in the collection
sentinel2 = sentinel2.map(mask_clouds)

# Generate a median composite of the images (to reduce the effect of cloud cover)
median_image = sentinel2.median()

# Clip the image to the study area
clipped_image = median_image.clip(study_area)

# Visualize the result (for example, using Folium or matplotlib)
# Convert Earth Engine image to a URL for visualization in Folium
url = clipped_image.getMapId()

# Create a map centered around your study area
map_center = [gdf.geometry.centroid.y.mean(), gdf.geometry.centroid.x.mean()]
m = folium.Map(location=map_center, zoom_start=10)

# Add the satellite image layer to the map
folium.TileLayer(
    tiles=url['tile_fetcher'].url_format,
    attr='Google Earth Engine',
    name='Sentinel-2',
    overlay=True,
    control=True
).add_to(m)

# Show the map
m

# Optionally: Export the image to Google Drive as a GeoTIFF for download
# Set the export task parameters
export_task = ee.batch.Export.image.toDrive(
    image=clipped_image,
    description='SOC_Dynamics_Sylhet_Basin',
    folder='GEE_Exports',
    fileNamePrefix='SOC_Sylhet_Basin',
    region=study_area,
    scale=10,  # Sentinel-2 has 10m resolution for some bands
    crs='EPSG:4326'
)

# Start the export task
export_task.start()


# Define the time period (last 10 years)
end_date = datetime.today()
start_date = end_date - timedelta(days=10*365)

# Convert start and end dates to strings in the required format
start_date_str = start_date.strftime('%Y-%m-%d')
end_date_str = end_date.strftime('%Y-%m-%d')

# Define the Sentinel-2 ImageCollection
sentinel2 = ee.ImageCollection("COPERNICUS/S2") \
    .filterBounds(study_area) \
    .filterDate(ee.Date(start_date_str), ee.Date(end_date_str)) \
    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 30))  # Filter to less than 30% cloud cover

# Select relevant bands (e.g., B4, B3, B2, B8 for Red, Green, Blue, NIR)
sentinel2 = sentinel2.select(['B4', 'B3', 'B2', 'B8'])

# Function to apply a cloud mask using NDVI-based approach or SCL
def mask_clouds(image):
    # Use NDVI or SCL-based cloud mask (SCL is available in Level 2A, or use a threshold for NDVI)
    # First calculate the NDVI for cloud masking
    ndvi = image.normalizedDifference(['B8', 'B4']).rename('NDVI')
    
    # Create a cloud mask: NDVI less than a certain threshold will likely be cloudy
    cloud_mask = ndvi.gt(0.5)  # Use NDVI > 0.2 for non-cloudy pixels (this is a threshold you can adjust)
    
    # Mask out clouds
    return image.updateMask(cloud_mask)

# Apply the cloud mask to all images in the collection
sentinel2 = sentinel2.map(mask_clouds)

# Loop through each year in the 5-year range and export each year individually
for year in range(start_date.year, end_date.year + 1):
    # Filter the image collection for the current year
    year_start = f"{year}-01-01"
    year_end = f"{year}-12-31"

    # Filter the ImageCollection for the current year
    sentinel2_year = sentinel2.filterDate(ee.Date(year_start), ee.Date(year_end))
    
    # Generate a median composite of the images (to reduce the effect of cloud cover)
    median_image = sentinel2_year.median()

    # Clip the image to the study area
    clipped_image = median_image.clip(study_area)

    # Export the image to Google Drive as a GeoTIFF for each year
    export_task = ee.batch.Export.image.toDrive(
        image=clipped_image,
        description=f'SOC_Dynamics_{year}',  # Description with year
        folder='GEE_Exports',  # Google Drive folder
        fileNamePrefix=f'SOC_Sylhet_Basin_{year}',  # File name prefix with year
        region=study_area,
        scale=10,  # Sentinel-2 has 10m resolution for some bands
        crs='EPSG:4326',
        fileFormat='GeoTIFF'
    )
    
    # Start the export task
    export_task.start()

    print(f"Export task started for year {year}. Monitor its progress in Earth Engine Task Manager.")



